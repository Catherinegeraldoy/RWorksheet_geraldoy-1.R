---
title: "RWorksheet_6"
author: "Catherine Geraldoy BSIT 2-B"
date: "2023-12-14"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
##1. Create a data frame for the table below. Show your solution.

students_data <- data.frame (
  Students = c(1,2,3,4,5,6,7,8,9,10),
  preTest =  c(55,54,47,57,51,61,57,54,63,58),
  postTest = c(61,60,56,63,56,63,59,56,62,61)
)

#a. Compute the descriptive statistics using different packages (Hmisc and pastecs).Write the codes and its result

#install.packages("Hmisc")
library(Hmisc)
#install.packages("pastecs")
library(pastecs)

stats_hmisc<-describe(students_data)
stats_pastics <- stat.desc(students_data)

#2. The Department of Agriculture was studying the effects of several levels of a fertilizer on the growth of a plant. For some analyses, it might be useful to convert the fertilizer levels to an ordered factor.

#a. Write the codes and describe the result.
data_fertilize <- c(10,10,10, 20,20,50,10,20,10,50,20,50,20,10)
ordered(data_fertilize)
# The result of the 'ordered' function applied to the 'data_fertilize' vector indicates that the fertilizer levels have been successfully transformed into an ordered factor. This transformation is beneficial for certain analyses, providing a structured representation of the varying levels of fertilizer used in the study. The ordered factor ensures a logical and meaningful arrangement, enhancing the interpretability of the data in the context of the Department of Agriculture's investigation into the effects of different fertilizer levels on plant growth.


#3. Abdul Hassan, president of Floor Coverings Unlimited, has asked you to study the ex-ercise levels undertaken by 10 subjects were “l”, “n”, “n”, “i”, “l” , “l”, “n”,“n”, “i”, “l” ; n=none, l=light, i=intense.

# a. What is the best way to represent this in R?

exe_levels <- c("l", "n", "n", "i", "l", "l", "n", "n", "i", "l")
exercise_factor <- factor(exe_levels, levels = c("n", "l", "i"), labels = c("none", "light", "intense"))
exercise_factor

# 4. Sample of 30 tax accountants from all the states and territories of Australia and their individual state of origin is specified by a character vector of state mnemonics as:


tax_accountants <- c("tas", "sa", "qld", "nsw", "nsw", "nt", "wa", "wa", "qld",
           "vic", "nsw", "vic", "qld", "qld", "sa", "tas", "sa", "nt",
           "wa", "vic", "qld", "nsw", "nsw", "wa", "sa", "act", "nsw",
           "vic", "vic", "act")

factor_level <-factor(tax_accountants, levels = c("act", "nsw", "nt", "qld", "sa", "tas", "vic", "wa") )  
factor_level
#the factor_level variable result is factor with level.

# 5. From #4 - continuation:
# • Suppose we have the incomes of the same tax accountants in another vector (in suitably large units of money)

incomes <- c(60, 49, 40, 61, 64, 60, 59, 54,
             62, 69, 70, 42, 56, 61, 61, 61, 58, 51, 48,
             65, 49, 49, 41, 48, 52, 46, 59, 46, 58, 43)

# a. Calculate the sample mean income for each state we can now use the special function tapply():

mean_income <- tapply(incomes, factor_level, mean)

mean_income

# b. Copy the results and interpret.
#The result has the means of each states that has factor with levels 
# act      nsw       nt      qld       sa      tas      vic       wa 
#50000 57.33333 55.50000 53.60000 55.00000 60.50000 56.00000 52.25000

#6. Calculate the standard errors of the state income means (refer again to number 3) 
#stdError <- function(x) sqrt(var(x)/length(x)) Note: After this assignment, the standard errors are calculated by: incster <- tapply(incomes, statef, stdError)

#a. What is the standard error? Write the codes.
stdError <- function(x) sqrt(var(x)/length(x))
incster <- tapply(incomes, factor_level, stdError)
incster

#b. Interpret the result.
#answer. The outcome presents the computed standard errors associated with the mean incomes of different states. A smaller standard error implies a more precise estimation of the actual population mean income for that state based on the sample mean income. Conversely, a larger standard error indicates greater variability in the sample mean, reflecting less precision in estimating the genuine population mean.
#7. Use the titanic dataset.

#a. subset the titatic dataset of those who survived and not survived. Show the codes and its result.
library(datasets)
data(Titanic)

Titanic<-as.data.frame(Titanic)

survived<-subset(Titanic, Survived=="Yes")
survived

not_survived <- subset(Titanic, Survived == "No")
not_survived

#8. The data sets are about the breast cancer Wisconsin. The samples arrive periodically as Dr. Wolberg reports his clinical cases. The database therefore reflects this. 
library(readr)
csv.file<-"breastcancer_wisconsin.csv"
breastcancer_wisconsin<-read.csv("breastcancer_wisconsin.csv")
breastcancer_wisconsin

#a. describe what is the dataset all about.
#answer.The 'breastcancer_wisconsin' dataset comprises clinical records of cases, providing identification numbers along with diverse categories indicating the severity of breast cancer. The malignancy levels are quantified on a numerical scale, ranging from 1 to 10.
#d. Compute the descriptive statistics using different packages. Find the values of:
#d.1 Standard error of the mean for clump thickness.
#Using stdError function
clumpthcknss_data <- breastcancer_wisconsin$clump_thickness
std_error_clump_thickness <- stdError(clumpthcknss_data)
std_error_clump_thickness
#0.1065011

#d.2 Coefficient of variability for Marginal Adhesion.
#Using mean and standard deviation to get the Coefficient of Variation. 
marginal_adhesion <- breastcancer_wisconsin$marginal_adhesion
mean <- mean(marginal_adhesion)
sd <- sd(marginal_adhesion)
cv <- sd / mean
cv
cv<-cv*100 
cv

#d.3 Number of null values of Bare Nuclei.
bare_nuclei <- breastcancer_wisconsin$bare_nucleoli
num_null <- sum(is.na(bare_nuclei))
num_null

#d.4 Mean and standard deviation for Bland Chromatin
#Using mean and standard deviation
bland_chromatin_data <- breastcancer_wisconsin$bland_chromatin
mean_bland_chromatin <- mean(bland_chromatin_data)
sd_bland_chromatin <- sd(bland_chromatin_data)
mean_bland_chromatin
sd_bland_chromatin

#d.5 Confidence interval of the mean for Uniformity of Cell Shape
#Using t.test function
uniformity_cellShape_data <- breastcancer_wisconsin$shape_uniformity
confidence_interval <- t.test(uniformity_cellShape_data, na.rm = TRUE)$conf.int
print(confidence_interval)

#d. How many attributes?
length(breastcancer_wisconsin)
names(breastcancer_wisconsin)

#e. Find the percentage of respondents who are malignant. Interpret the results

malignant_percentage <- sum(breastcancer_wisconsin$class == 4) / nrow(breastcancer_wisconsin) * 100
malignant_percentage

#9. Export the data abalone to the Microsoft excel file. Copy the codes.
install.packages("AppliedPredictiveModeling")
library("AppliedPredictiveModeling")
data("abalone")
head(abalone)
summary(abalone)

getwd()  
Abalone_excel<-"/cloud/project/RWorksheet_6/AbaloneData.xlsx"
#install.packages("writexl")
library(writexl)

write_xlsx(abalone, Abalone_excel)
```

